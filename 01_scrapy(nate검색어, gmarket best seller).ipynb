{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapy\n",
    "- 파이썬 언어를 이용한 웹 데이터 수집 프레임워크\n",
    "    - 프레임워크, 라이브러리 또는 패키지의 차이\n",
    "    - 프레임워크: 특정 목적을 가진 기능의 코드가 미리 설정되어서 빈칸 채우기 식으로 코드를 작성하는 방식.\n",
    "    - 패키지: 다른 사람이 작성해놓은 코드를 가져다가 사용하는 방법\n",
    "        \n",
    "- scrapy\n",
    "  - pip install scrapy\n",
    "    \n",
    "- tree\n",
    "  - sudo apt install tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Index\n",
    " - xpath: css-selector 역할을 해주는 문법(셀레니움에서도 xpath가 있긴하나, 사용되는게 좀 다름.)\n",
    " - 스크래피의 구조\n",
    " - gmarket 베스트 상품 데이터 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import requests\n",
    "from scrapy.http import TextResponse #xpath 연습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. xpath 사용법\n",
    "- naver실시간 검색어 데이터\n",
    "- 검색어 xpath\n",
    "\n",
    "```\n",
    "//*[@id=\"NM_RTK_ROLLING_WRAP\"]/ul/li[19]/a/span[2]\n",
    "```\n",
    "\n",
    "- `//` : 가장 상위 엘리먼트\n",
    "- `*` : 조건에 맞는 하위 엘리먼트를 모두 살펴봄, css selector에서는 띄어쓰기 \" \" 와 같음. \"div .txt\"\n",
    "- `[@id=\"NM_RTK_ROLLING_WRAP\"]` : 조건 : id가 PM_ID_ct인 엘리먼트\n",
    "- `/`: 바로 아래 엘리먼트를 살펴봄. \"div > .txt\"\n",
    "- `li[19]` : li 태그에서 19번째 엘리먼트를 선택\n",
    "- `span[2]`: span 태그에서 2번째 엘리먼트를 선택\n",
    "\n",
    "- ※ 참고\n",
    "- `.`: 현재 엘리먼트를 선택\n",
    "- `not` : not(조건)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nate 검색어 가져오기 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nate 웹 페이지에 연결\n",
    "req = requests.get(\"https://search.daum.net/nate?w=tot&DA=R02&q=%EB%AF%B8%EC%8A%A4%ED%84%B0%ED%8A%B8%EB%A1%AF&rtmaxcoll=1ET,TVP&irt=tv-program&irk=86091\")\n",
    "\n",
    "# response 객체 생성\n",
    "response = TextResponse(req.url, body=req.text, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Selector xpath='//*[@id=\"ratThemeColl\"]/div[2]/div/ol[1]/li[1]/div/span[2]/a/b' data='<b>미스터트롯</b>'>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 네이트 예능 tv 검색어 순위 데이터 가져오기\n",
    "#xpath: xpath selector\n",
    "# data : xpath selector로 선택된 엘리먼트\n",
    "response.xpath('//*[@id=\"ratThemeColl\"]/div[2]/div/ol[1]/li[1]/div/span[2]/a/b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Selector xpath='//*[@id=\"ratThemeColl\"]/div[2]/div/ol[1]/li[1]/div/span[2]/a/b/text()' data='미스터트롯'>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text를 data로 설정\n",
    "response.xpath('//*[@id=\"ratThemeColl\"]/div[2]/div/ol[1]/li[1]/div/span[2]/a/b/text()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response 객체에서 data 변수만 가져옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['미스터트롯']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.xpath('//*[@id=\"ratThemeColl\"]/div[2]/div/ol[1]/li[1]/div/span[2]/a/b/text()').extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['슈퍼맨이 돌아왔다', '뭉쳐야 찬다']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 네이버 실시간 데이터 한번에 가져오기\n",
    "response.xpath('//*[@id=\"ratThemeColl\"]/div[2]/div/ol/li/div/span[2]/a/text()').extract()[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Scrapy Project\n",
    "- scrapy 프로젝트 생성\n",
    "- scrapy 구조\n",
    "- gmarket 베스트 상품 링크 수집, 링크 안에 있는 상세 정보 수집"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 프로젝트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: scrapy.cfg already exists in C:\\Code\\04 Web\\scrapy\\crawler\n"
     ]
    }
   ],
   "source": [
    "!scrapy startproject crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00_iterator_generator.ipynb\n",
      "01_scrapy_gmarket.ipynb\n",
      "03_scrapy_naver_movie.ipynb\n",
      "crawler\n",
      "naver_movie\n",
      "run.sh\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "폴더 PATH의 목록입니다.\n",
      "볼륨 일련 번호는 82F2-BFE3입니다.\n",
      "C:\\CODE\\04 WEB\\SCRAPY\\CRAWLER\n",
      "└─crawler\n",
      "    ├─spiders\n",
      "    │  └─__pycache__\n",
      "    └─__pycache__\n"
     ]
    }
   ],
   "source": [
    "!tree crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scrapy의 구조\n",
    "- spiders\n",
    "    - 어떤 웹서비스를 어떻게 크롤링할 것인지에 대한 코드를 작성(.py 파일로 작성)\n",
    "\n",
    "- items.py\n",
    "    - 모델에 해당하는 코드, 저장하는 데이터의 자료구조를 설정\n",
    "\n",
    "- pipelines.py\n",
    "    - 스크래핑한 결과물을 item 형태로 구성하고 처리하는 방법에 대한 코드\n",
    "    \n",
    "- settings.py\n",
    "    - 스크래핑 할때의 환경 설정값을 지정\n",
    "    - robots.txt: 따를지, 안따를지\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gmarket 베스트 셀러 상품 수집\n",
    "- 상품명, 상세페이지 URL, 원가, 판매가, 할인율\n",
    "- xpath 확인\n",
    "- items.py를 수집하려는 데이터에 맞게끔 설정\n",
    "- spider.py를 작성\n",
    "- 크롤러 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. xpath 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = requests.get(\"http://corners.gmarket.co.kr/Bestsellers\")\n",
    "response = TextResponse(req.url, body=req.text, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links= response.xpath('//*[@id=\"gBestWrap\"]/div/div[3]/div[2]/ul/li/div[1]/a/@href').extract()\n",
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://item.gmarket.co.kr/Item?goodscode=751246244&ver=637187559854100933'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('쏭스타일 신상추가.맨투맨.루즈핏.롱티 ', '12900', '43000', '70.0%')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "req = requests.get(links[1])\n",
    "response = TextResponse(req.url, body=req.text, encoding=\"utf-8\")\n",
    "\n",
    "title = response.xpath('//*[@id=\"itemcase_basic\"]/h1/text()')[0].extract()\n",
    "o_price = response.xpath('//*[@id=\"itemcase_basic\"]/p/span/span/text()')[0].extract().replace(\",\",\"\")\n",
    "s_price = response.xpath('//*[@id=\"itemcase_basic\"]/p/span/strong/text()')[0].extract().replace(\",\",\"\")\n",
    "discount_rate = str(round((1 - int(s_price)/int(o_price))*100,2)) + \"%\"\n",
    "\n",
    "title, s_price, o_price, discount_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. items.py 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import scrapy\n",
      "\n",
      "class CrawlerItem(scrapy.Item):\n",
      "    title = scrapy.Field()\n",
      "    o_price = scrapy.Field()\n",
      "    s_price = scrapy.Field()\n",
      "    discount_rate = scrapy.Field()\n",
      "    link=scrapy.Field()\n"
     ]
    }
   ],
   "source": [
    "!cat crawler/crawler/items.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting crawler/crawler/items.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile crawler/crawler/items.py\n",
    "\n",
    "import scrapy\n",
    "\n",
    "class CrawlerItem(scrapy.Item):\n",
    "    title = scrapy.Field()\n",
    "    o_price = scrapy.Field()\n",
    "    s_price = scrapy.Field()\n",
    "    discount_rate = scrapy.Field()\n",
    "    link=scrapy.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. spider.py 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting crawler/crawler/spiders/spider.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile crawler/crawler/spiders/spider.py\n",
    "import scrapy\n",
    "from crawler.items import CrawlerItem\n",
    "\n",
    "class Spider(scrapy.Spider):\n",
    "    name = \"GmarketBestsellers\"\n",
    "    allow_domain = [\"gmarket.co.kr\"]# 이 도메인에 있는 url만 크롤링하겠다.\n",
    "    start_urls = [\"http://corners.gmarket.co.kr/Bestsellers\"]#최초에 request해주는 url\n",
    "    \n",
    "    def parse(self, response):#start_urls에 작성되어있는 걸로 scrapy가 request하면 그 response 받아서 이 함수 실행시켜줌. \n",
    "    # request, response해서 함수 실행시키는거는 scrapy 프레임워크안에 이미 들어있어서, 아래만 작성해주면 됨.\n",
    "        links = response.xpath('//*[@id=\"gBestWrap\"]/div/div[3]/div[2]/ul/li/div[1]/a/@href').extract()#link 200개 가져옴\n",
    "        for link in links:\n",
    "            yield scrapy.Request(link, callback=self.page_content)#함수가 200번 실행되면서 순서대로 200개의 다른 링크 실행됨.\n",
    "            \n",
    "    def page_content(self, response):#200개 링크 실행되면서 page_content 함수 호출.(상세 페이지에 대한 response가 들어감)\n",
    "        item = CrawlerItem()#상세 페이지의 데이터를 item 객체에 저장\n",
    "        item[\"title\"] = response.xpath('//*[@id=\"itemcase_basic\"]/h1/text()')[0].extract()\n",
    "        item[\"s_price\"] = response.xpath('//*[@id=\"itemcase_basic\"]/p/span/strong/text()')[0].extract().replace(\",\", \"\")\n",
    "        try:#original price 없는 상품들 에러나서 예외 처리\n",
    "            item[\"o_price\"] = response.xpath('//*[@id=\"itemcase_basic\"]/p/span/span/text()')[0].extract().replace(\",\", \"\")\n",
    "        except:\n",
    "            item[\"o_price\"] = item[\"s_price\"]\n",
    "        item[\"discount_rate\"] = str(round((1 - int(item[\"s_price\"]) / int(item[\"o_price\"]))*100, 2)) + \"%\"\n",
    "        item[\"link\"] = response.url\n",
    "        yield item\n",
    "        \n",
    "#scrapy 프로젝트 실행했을 때, Spider 클래스가 객체가 되면서 start_url로 request\n",
    "# response 받아와서 parse 함수 실행 -> 베스트셀러 200개 있는 페이지에서 링크 데이터 200개 가져옴. 링크 데이터로 yield로 scrapy.Request하면서 200번 싷행하고나서 결과 데이터가 link로 요청하고, call back 함수(page_content) 200번 실행.\n",
    "# 상세페이지에 대한 데이터 받아와서 item 객체 만들어줌. 200번 실행되면서 item이 결과로 출력됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. scrapy 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawler\n",
      "GmarketBestsellers.csv\n",
      "scrapy.cfg\n"
     ]
    }
   ],
   "source": [
    "!ls crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile run.sh\n",
    "cd crawler\n",
    "scrapy crawl GmarketBestsellers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "!run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load crawler/crawler/spiders/spider.py : 위의 Spider class 코드 나옴."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 csv로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile run.sh\n",
    "cd crawler\n",
    "scrapy crawl GmarketBestsellers -o GmarketBestsellers.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "!run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawler\n",
      "GmarketBestsellers.csv\n",
      "scrapy.cfg\n"
     ]
    }
   ],
   "source": [
    "!ls crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/c/Code/04 Web/scrapy\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['crawler', 'GmarketBestsellers.csv', 'scrapy.cfg']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files= !ls crawler/\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discount_rate</th>\n",
       "      <th>link</th>\n",
       "      <th>o_price</th>\n",
       "      <th>s_price</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0%</td>\n",
       "      <td>http://item.gmarket.co.kr/Item?goodscode=17591...</td>\n",
       "      <td>29900</td>\n",
       "      <td>29900</td>\n",
       "      <td>[아웃팅] 정전기필터 마스크 필터 50매 한박스 일회용마스크</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.2%</td>\n",
       "      <td>http://item.gmarket.co.kr/Item?goodscode=17231...</td>\n",
       "      <td>43500</td>\n",
       "      <td>39500</td>\n",
       "      <td>[순수식품] 남극 크릴오일1000 3박스(총 90캡슐) 인지질58%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.14%</td>\n",
       "      <td>http://item.gmarket.co.kr/Item?goodscode=16594...</td>\n",
       "      <td>14000</td>\n",
       "      <td>10900</td>\n",
       "      <td>[트레비] 트레비 레몬 300ml 20pet /1박스</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0%</td>\n",
       "      <td>http://item.gmarket.co.kr/Item?goodscode=16103...</td>\n",
       "      <td>17800</td>\n",
       "      <td>17800</td>\n",
       "      <td>[맥콜] 맥콜 160ml 30캔x2 (총60캔) / 탄산음료</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68.87%</td>\n",
       "      <td>http://item.gmarket.co.kr/Item?goodscode=38431...</td>\n",
       "      <td>31800</td>\n",
       "      <td>9900</td>\n",
       "      <td>코방콕 하는 어린이를 위한 보드게임 베스트</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  discount_rate                                               link o_price  \\\n",
       "0          0.0%  http://item.gmarket.co.kr/Item?goodscode=17591...   29900   \n",
       "1          9.2%  http://item.gmarket.co.kr/Item?goodscode=17231...   43500   \n",
       "2        22.14%  http://item.gmarket.co.kr/Item?goodscode=16594...   14000   \n",
       "3          0.0%  http://item.gmarket.co.kr/Item?goodscode=16103...   17800   \n",
       "4        68.87%  http://item.gmarket.co.kr/Item?goodscode=38431...   31800   \n",
       "\n",
       "  s_price                                   title  \n",
       "0   29900      [아웃팅] 정전기필터 마스크 필터 50매 한박스 일회용마스크   \n",
       "1   39500  [순수식품] 남극 크릴오일1000 3박스(총 90캡슐) 인지질58%   \n",
       "2   10900          [트레비] 트레비 레몬 300ml 20pet /1박스   \n",
       "3   17800      [맥콜] 맥콜 160ml 30캔x2 (총60캔) / 탄산음료   \n",
       "4    9900                코방콕 하는 어린이를 위한 보드게임 베스트   "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"crawler/{}\".format(files[1]))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. pipelines 설정\n",
    "-item을 출력하기 전에 실행되는 코드를 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def send_slack(msg):\n",
    "    WEBHOOK_URL = \"https://hooks.slack.com/services/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\"\n",
    "    payload = {\n",
    "        \"channel\": \"#test\",\n",
    "        \"username\": \"LHJ\",\n",
    "        \"text\": msg,\n",
    "    }\n",
    "    requests.post(WEBHOOK_URL, json.dumps(payload))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "send_slack(\"테스트\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "class CrawlerPipeline(object):\n",
      "    def process_item(self, item, spider):\n",
      "        \n",
      "        def send_slak(mag):\n",
      "        WEBHOOK_URL=\"https://hooks.slack.com/services/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\"\n",
      "        payload={\n",
      "        \"channel\" : \"#test\",\n",
      "        \"username\": \"LHJ\",\n",
      "        \"text\" : \"mag\",\n",
      "        }\n",
      "        requests.post(WEBHOOK_URL, json.dumps(payload))\n",
      "        \n",
      "        def process_item(self, item, spider):\n",
      "            keyword = \"�꽭�듃\"\n",
      "            print(\"=\"*100)\n",
      "            print(item[\"title\"])\n",
      "            print(\"=\"*100)\n",
      "            if keyword in item[\"title\"]:\n",
      "                self.__send_slack(\"{},{},{}\".format(item[\"title\"], item[\"s_price\"], item[\"link\"] ))\n",
      "            \n",
      "        \n",
      "        return item\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "!cat crawler/crawler/pipelines.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting crawler/crawler/pipelines.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile crawler/crawler/pipelines.py\n",
    "\n",
    "class CrawlerPipeline(object):\n",
    "    def process_item(self, item, spider):\n",
    "        \n",
    "        def send_slak(mag):\n",
    "        WEBHOOK_URL=\"https://hooks.slack.com/services/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\"\n",
    "        payload={\n",
    "        \"channel\" : \"#test\",\n",
    "        \"username\": \"LHJ\",\n",
    "        \"text\" : \"mag\",\n",
    "        }\n",
    "        requests.post(WEBHOOK_URL, json.dumps(payload))\n",
    "        \n",
    "        def process_item(self, item, spider):\n",
    "            keyword = \"세트\"\n",
    "            print(\"=\"*100)\n",
    "            print(item[\"title\"])\n",
    "            print(\"=\"*100)\n",
    "            if keyword in item[\"title\"]:\n",
    "                self.__send_slack(\"{},{},{}\".format(item[\"title\"], item[\"s_price\"], item[\"link\"] ))\n",
    "            \n",
    "        \n",
    "        return item\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pipeline 설정: settings.py\n",
    "\n",
    "```\n",
    "ITEM_PIPELINES = {\n",
    "    'crawler.pipelines.CrawlerPipeline': 300,\n",
    "}\n",
    "```\n",
    "#파이프라인 실행되는 순서,숫자가 낮을수록 빨리 실행딤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"ITEM_PIPELINES = {\" >> crawler/crawler/settings.py #echo 뒤에 글자 출력, >> 얘가 출력 역할\n",
    "!echo \"    'crawler.pipelines.CrawlerPipeline': 300,\"  >> crawler/crawler/settings.py\n",
    "!echo \"}\"  >> crawler/crawler/settings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"}\"  \n",
      "\"    'crawler.pipelines.CrawlerPipeline': 300,\"  \n",
      "\"}\"  \n"
     ]
    }
   ],
   "source": [
    "!tail -n 3 crawler/crawler/settings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "!run.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "project(crawler)\n",
    "\n",
    "- spider: crawling 절차에 대한 코드 들어있음.\n",
    "- items.py : 크롤링할 데이터의 모델을 정해줌.(여기선 5가지.. title, s_price, o_price..)\n",
    " - parse라고 하는 함수 이름은 바꿀 수 없음.\n",
    "\n",
    "- pipeline.py: 실행하기 전에 코드를 실행해줌\n",
    "- settings.py: 환경 설정"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
